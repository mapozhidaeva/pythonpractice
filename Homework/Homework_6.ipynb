{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Следующее задание такое.\n",
    "\n",
    "Нужно выбрать для себя новостной сайт.\n",
    "Заведите себе гуглдок и запишите туда свои сайты -- чтобы не повторяться.\n",
    "\n",
    "В репозитории должен лежать готовый код, который умеет делать всё, что я перечислю ниже, а также в файле Readme.md в папке с проектом должна находиться ссылка на скачанные и обработанные файлы вашего новостного сайта, лежащие архивом на Яндекс.Диске, Google Drive, Облаке@Мэйл.ру или на чём угодно подобном.\n",
    "При этом \n",
    "1. Период, за который должны быть тексты: 1 июля 2015 -- 31 декабря 2018 года.\n",
    "2. Общий объём скачанных текстов должен быть не меньше 5 млн. слов (можно больше). \n",
    "3. В именах файлов не должно быть не-ascii символов.\n",
    "4. Структура каталогов, в которых хранятся файлы, должна быть следующей: корень/год/месяц/файлы с материалами за этот месяц\n",
    "5. В корне также должна лежать csv-таблица с полями, которые я перечислю ниже. \n",
    "\n",
    "Соответственно, сама программа (в одном или в нескольких файлах) должна уметь:\n",
    "1. написать программу, которая умеет автоматически обращаться к сайту газеты и скачивать оттуда html-страницы, при этом программа должна уметь находить ссылки на другие страницы этого сайта и скачивать их тоже.\n",
    "При этом программа не должна заходить на одни и те же страницы несколько раз.\n",
    "2. с помощью lxml (или другого парсера xml) извлекать со страниц информацию (если она доступна) для метатаблицы и сам текст.\n",
    "Метатаблица должна иметь следующие поля: \n",
    "path;author;date;source;title;url;wordcount\n",
    "path -- это путь к файлу. \n",
    "source -- это название новостного сайта. \n",
    "wordcount -- число слов в тексте.\n",
    "Остальное, думаю, понятно (если вдруг непонятно, пишите).\n",
    "3. раскладывать скачанные тексты по папкам (в соответствии с теми правилами, которые изложены выше).\n",
    "4. вызывать mystem и делать морфологическую разметку текста (таким образом, чтобы каждому слову была присвоена информация о лемме и грамматическая информация с учётом контекстно снятой омонимии).\n",
    "Размеченные майстемом файлы должны лежать в отдельной папке. То есть структура каталога должна быть такая:\n",
    "\n",
    "корень\n",
    "|\n",
    "|___ plain text\n",
    "|      |___год\n",
    "|            |___месяц\n",
    "|___ размеченные майстемом тексты \n",
    "       |___год\n",
    "             |___месяц\n",
    "Файлы, размеченные майстемом, записывать в метатаблицу не надо.\n",
    "\n",
    "Сделать всё нужно до 23:59 28 февраля 2019 года.\n",
    "\n",
    "Всего доброго,\n",
    "Борис Орехов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import re\n",
    "import html5lib\n",
    "import urllib.request\n",
    "import os\n",
    "import pandas as pd\n",
    "from pymystem3 import Mystem\n",
    "m = Mystem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "site = 'https://hi-news.ru/'\n",
    "sitemap = 'https://hi-news.ru/sitemap.xml'\n",
    "end_date = '2015-07'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_sitemap(sitemap, end_date):\n",
    "    # path;author;date;source;title;url;wordcount\n",
    "    # path: re.sub('\\-', '/', date[:-3])\n",
    "    # \n",
    "    \n",
    "    end_date = int(''.join(re.findall('\\d+', end_date)))\n",
    "    print (end_date)\n",
    "    resp = requests.get(sitemap)\n",
    "    links = str(bs(resp.text, 'lxml'))\n",
    "    links = re.findall('https[^<]+\\d\\.xml', links)\n",
    "    articles = []\n",
    "    count_links = 0\n",
    "    for link in links:\n",
    "        date = int(''.join(re.findall('\\d+', link)))\n",
    "        if date >= end_date:\n",
    "            count_links += 1\n",
    "            arts = requests.get(link)\n",
    "            arts = str(bs(arts.text, 'lxml'))\n",
    "            arts = re.findall('https[^<]+\\.html', arts)\n",
    "            articles += arts\n",
    "    print ('{} xml links parsed'.format(count_links))\n",
    "    print ('{} articles collected'.format(len(articles)))\n",
    "    return articles    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201507\n",
      "48 xml links parsed\n",
      "10532 articles collected\n"
     ]
    }
   ],
   "source": [
    "links = parse_sitemap(sitemap, '2015-07')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def pos(text):\n",
    "    pos_text = ''\n",
    "    for a in m.analyze(text):\n",
    "        \n",
    "        try:\n",
    "            text = a['text']\n",
    "            lemma = a['analysis'][0]['lex']\n",
    "            gr = a['analysis'][0]['gr']\n",
    "            if a['text'].lower() == lemma:\n",
    "                lemma = ''\n",
    "            if '|' in gr:\n",
    "                gr = ''\n",
    "\n",
    "            output = str([i for i in [lemma, gr] if len(i) > 0])\n",
    "            if len(output) == 0:\n",
    "                output = ''\n",
    "            pos_text += text + output\n",
    "        except KeyError:\n",
    "            pos_text += a['text']\n",
    "        except IndexError:\n",
    "            pos_text += a['text']\n",
    "    return pos_text.strip('\\n')\n",
    "\n",
    "def parse_articles(list_of_articles):\n",
    "    df = pd.DataFrame(columns=['path','date','source','title','url','wordcount'])\n",
    "    path = os.path.abspath('.') + '/news/'\n",
    "    for article in list_of_articles:\n",
    "        art_text = requests.get(article).text\n",
    "        \n",
    "        #art_text = requests.get(links[0]).text\n",
    "        \n",
    "        date = re.findall('\\\"datePublished\": \\\"(\\d{4}-\\d\\d-\\d\\d)', art_text)[0]\n",
    "        #print (date)\n",
    "        page = requests.get(article)\n",
    "        art_text_clean = bs(' '.join([re.sub('[ ]{2,}', '', p.get_text()) for p in bs(page.text, 'lxml').\\\n",
    "                     find_all('p')]), 'lxml').get_text().replace('\\xa0', ' ').replace('\\n', '')\n",
    "\n",
    "        #art_text_clean = re.findall('<div class=\"text\">([^{]+){', art_text)#[0]\n",
    "\n",
    "        title = re.findall('<title>([^<]+)</title>', art_text)[0][:-13]\n",
    "        file_title = re.sub('[^a-zа-яё]+', '_', title.lower()).strip('_')\n",
    "        #print (title)\n",
    "        author = re.findall('class=\\\"author[^>]+>([^<]+)<', art_text)[0]\n",
    "        #print (author)\n",
    "        #print (art_text_clean)\n",
    "        dir_path = path + 'plain_text' + '/' +  re.sub('-', '/', date)[:-3]\n",
    "        file_path =  dir_path + '/' + file_title + '.txt'\n",
    "        mystem_path = path + 'pos_tag' + '/' +  re.sub('-', '/', date)[:-3]\n",
    "        #print (file_path)\n",
    "        \n",
    "        try:\n",
    "            os.makedirs(dir_path, exist_ok=True)\n",
    "        except IsADirectoryError:\n",
    "            pass\n",
    "        except FileExistsError:\n",
    "            pass\n",
    "        \n",
    "        f = open(file_path, 'w')\n",
    "        f.write(art_text_clean)\n",
    "        f.close()\n",
    "        wordcount = len(re.sub('[^a-zа-яё ]', '', art_text_clean.lower()).split())\n",
    "        df = df.append({'path':file_path,'date':date,'source':site,'title':title,\n",
    "                        'url':article,'wordcount':wordcount}, ignore_index=True)\n",
    "        \n",
    "        pos_text = pos(art_text_clean)\n",
    "        try:\n",
    "            os.makedirs(mystem_path, exist_ok=True)\n",
    "        except IsADirectoryError:\n",
    "            pass\n",
    "        except FileExistsError:\n",
    "            pass\n",
    "        \n",
    "        f = open(mystem_path + '/' + file_title + '.txt', 'w')\n",
    "        f.write(pos_text)\n",
    "        f.close()\n",
    "        \n",
    "    return df\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time df = parse_articles(links)\n",
    "df.to_csv('hi_news_articles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10532"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
